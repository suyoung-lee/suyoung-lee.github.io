<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Suyoung Lee</title>

  <meta name="author" content="Suyoung Lee">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/SY.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Suyoung Lee</name>
                              </p>
              
		<p style="text-align:center"> suyounglee424 [at] gmail [dot] com </p>
		    <p style="text-align:center"> suyoung.lee [at] krafton [dot] com </p>
              </p>
              <p> Welcome to my homepage. My name is Suyoung Lee, and I currently work as an ML engineer at the Gameplay AI Team, Krafton. My current interest is building AI agents that can play games {like, with, against} humans.
				  <br><br>
		Before joining Krafton, I worked on building LLM agent frameworks at Samsung.
				  <br><br>
		I completed my PhD in Electrical Engineering at KAIST, under the supervision of Prof. <a href="https://sites.google.com/view/youngchulsung">Youngchul Sung</a> and Prof. Sae-Young Chung.
                My research interest during PhD was in enhancing the practicality of reinforcement learning, including sample efficiency, exploration methods, generalization across unseen tasks, and refining offline reinforcement learning techniques. 

              <p style="text-align:center">
                <a href="https://scholar.google.com/citations?user=CWbdBy8AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/suyoung-lee/">Github</a> &nbsp/&nbsp
		<a href="https://github.com/suyoung-lee/suyoung-lee.github.io/raw/main/CV_SuyoungLee.pdf">CV</a> &nbsp/&nbsp
		<a href="https://github.com/suyoung-lee/suyoung-lee.github.io/raw/main/Defense_SuyoungLee.pdf">Thesis Slides</a>
              </p>

            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/SuyoungLee.jpg"><img style="width:70%;max-width:70%" alt="profile photo" src="images/SuyoungLee.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <heading>Publications</heading>

	  <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
		      <a href="images/DC.png"><img src='images/DC.png' width="190"></a>
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=4HKjezHU-w8">
                <papertitle> Decision ConvFormer: Local Filtering in MetaFormer is Sufficient for Decision Making </papertitle>
              </a>
              <br>
              <a href="https://www.beanie00.com/"> Jeonghye Kim</a>, <strong>Suyoung Lee</strong>, <a href="https://sites.google.com/view/wjkim1202/"> Woojun Kim</a>, and <a href="https://sites.google.com/view/youngchulsung">Youngchul Sung</a>

              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2024  as <strong> Spotlight presentation (366/7262= 5.0%) </strong>
	              <br>
              <em>Foundation Models for Decision Making (FMDM) Workshop at NeurIPS</em>, 2023.
              <br>
              <a href="https://openreview.net/forum?id=af2c8EaKl8">pdf</a>
              
              <p></p>
              <p> We propose Decision ConvFormer, a new decision-maker based on MetaFormer with three convolution filters for offline RL, which excels in decision-making by understanding local associations and has an enhanced generalization capability.</p>
            </td>
          </tr>
		
	  <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
		      <a href="images/sdvt.png"><img src='images/sdvt.png' width="190"></a>
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=JX6UloWrmE">
                <papertitle>Parameterizing Non-Parametric Meta-Reinforcement Learning Tasks via Subtask Decomposition </papertitle>
              </a>
              <br>
              <strong>Suyoung Lee</strong>, Myungsik Cho, and <a href="https://sites.google.com/view/youngchulsung">Youngchul Sung</a>

              <br>
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2023.
              <br>
              <a href="https://openreview.net/forum?id=JX6UloWrmE&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DNeurIPS.cc%2F2023%2FConference%2FAuthors%23your-submissions)">pdf</a>
	              /
              <a href="https://github.com/suyoung-lee/SDVT">code</a>
              <p></p>
              <p> We enhance the generalization capability of meta-reinforcement learning on tasks with non-parametric variability by decomposing the tasks into elementary subtasks and conducting virtual training.</p>
            </td>
          </tr>
		
          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
		      <a href="images/AIMDA.png"><img src='images/AIMDA.png' width="190"></a>
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=4HKjezHU-w8">
                <papertitle>Adaptive Intrinsic Motivation with Decision Awareness </papertitle>
              </a>
              <br>
              <strong>Suyoung Lee</strong> and
							<a href="http://itl.kaist.ac.kr/sychung/index.html">Sae-Young Chung</a>

              <br>
              <em>Decision Awareness in Reinforcement Learning Workshop at ICML</em>, 2022.
              <br>
              <a href="https://openreview.net/pdf?id=4HKjezHU-w8">pdf</a>
              
              <p></p>
              <p> We propose an intrinsic reward coefficient adaptation scheme equipped with intrinsic motivation awareness and adjusts the intrinsic reward coefficient online to maximize the extrinsic return.</p>
            </td>
          </tr>

          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
		      <a href='images/LDM.png'> <img src='images/LDM.png' width="190"> </a>
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://proceedings.neurips.cc/paper/2021/hash/e48e13207341b6bffb7fb1622282247b-Abstract.html">
                <papertitle>Improving Generalization in Meta-RL with Imaginary Tasks from Latent Dynamics Mixture</papertitle>
              </a>
              <br>
              <strong>Suyoung Lee</strong> and
							<a href="http://itl.kaist.ac.kr/sychung/index.html">Sae-Young Chung</a>

              <br>
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2021.
              <br>
              <a href="https://arxiv.org/pdf/2105.13524.pdf">pdf</a>
              /
              <a href="https://github.com/suyoung-lee/LDM">code</a>
              <p></p>
              <p> We train an RL agent with imaginary tasks generated from mixtures of learned latent dynamics to generalize to unseen test tasks.</p>
            </td>
          </tr>

          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
		      <a href ='images/EBU.jpg'> <img src='images/EBU.jpg' width="190"></a>
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://proceedings.neurips.cc/paper/2019/hash/e6d8545daa42d5ced125a4bf747b3688-Abstract.html">
                <papertitle>Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update</papertitle>
              </a>
              <br>
              <strong>Suyoung Lee</strong>,
							<a href="https://www.linkedin.com/in/sungik-choi-171a05137/?originalSubdomain=kr">Sungik Choi</a>, and
							<a href="http://itl.kaist.ac.kr/sychung/index.html">Sae-Young Chung</a>

              <br>
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2019.
              <br>
              <a href="https://arxiv.org/pdf/1805.12375.pdf">pdf</a>
              /
              <a href="https://github.com/suyoung-lee/Episodic-Backward-Update">code</a>
              <p></p>
              <p>A computationally efficient recursive deep reinforcement learning algorithm that allows sparse and delayed rewards to propagate directly through all transitions of the sampled episode.</p>
            </td>
          </tr>


        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <heading>Awards</heading>

          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">

              <p></p>
	      <p><strong>Outstanding Ph.D. Dissertation Award</strong> - Thesis: <em>Meta-Reinforcement Learning with Imaginary Tasks</em>, KAIST EE, 2024.</p>
              <p>Qualcomm-KAIST Innovation Awards 2018 - paper competition awards for graduate students, Qualcomm, 2018.</p>
              <p>Un Chong-Kwan Scholarship Award - for the achievement of excellence in the 2017 entrance examination, KAIST EE, 2017.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <heading>Education</heading>

          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">

              <p></p>
              <p>2022~ 2024: Ph.D. in Electrical Engineering, KAIST, Daejeon, Korea (advisor: Prof. <a href="https://sites.google.com/view/youngchulsung">Youngchul Sung</a>). </p>
	      <p>2019~2022: Ph.D. in Electrical Engineering, KAIST, Daejeon, Korea (advisor: Prof. <a href="http://itl.kaist.ac.kr/sychung/index.html">Sae-Young Chung</a>). </p>
              <p>2017~2019: M.S. in Electrical Engineering, KAIST, Daejeon, Korea (advisor: Prof. <a href="http://itl.kaist.ac.kr/sychung/index.html">Sae-Young Chung</a>). </p>
              <p>2012~2017: B.S. in Electrical Engineering, KAIST, Daejeon, Korea. </p>
              <p>2010~2012: Hansung Science High School, Seoul, Korea. </p>
	      <p>2007~2009: Tashkent International School, Tashkent, Uzbekistan. </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <heading>How I try to live</heading>

          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
	<td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
		      <a href="images/Life.jpg"><img src='images/Life.jpg' width="190"></a>
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">

              <p></p>
              <p>I view life as a meta-reinforcement learning task, reminiscent of the MuJoCo Ant-direction. Everyone has their own unique, albeit often obscured, optimal life direction T. The objective of life is to maximize the cumulative reward r=M·T, defined as the dot product of our chosen direction M (how we decide to live) and the unseen true direction T. I was fortunate to have guidance from two professors who instilled in me the importance of minimizing the angle 
∣θ∣ and maximizing the magnitude ∣M∣.</p>

            </td>
          </tr>
        </tbody></table>
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website template from <a href="https://jonbarron.info/">here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>

      </td>
    </tr>
  </table>
</body>

</html>
