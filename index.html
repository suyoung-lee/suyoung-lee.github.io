<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Suyoung Lee</title>

  <meta name="author" content="Suyoung Lee">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/SY.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Suyoung Lee</name>
                              </p>
              <p style="text-align:center"> suyoung.l [at] kaist [dot] ac [dot] kr </p>
              </p>
              <p>Hello, I’m Suyoung Lee. I’m a Ph.D. student at KAIST. I’m currently under the supervision of Prof. <a href="https://sites.google.com/view/youngchulsung">Youngchul Sung</a> at
                Smart Information Systems Research Lab (<a href="https://sites.google.com/view/sisrelkaist/introduction">SISReL</a>) (my past advisor: Prof. Sae-Young Chung).
                My research interest is in enhancing the practicality of reinforcement learning. Specifically, I am focused on improving sample efficiency, exploration, and generalization on unseen tasks.

              <p style="text-align:center">
                <a href="https://scholar.google.com/citations?user=CWbdBy8AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/suyoung-lee/">Github</a> &nbsp/&nbsp
		<a href="https://github.com/suyoung-lee/suyoung-lee.github.io/raw/main/CV_SuyoungLee.pdf">CV</a>
              </p>

            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/SuyoungLee (2).jpg"><img style="width:70%;max-width:70%" alt="profile photo" src="images/SuyoungLee (2).jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <heading>Publications</heading>

	  <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
		      <a href="images/sdvt.png"><img src='images/sdvt.png' width="190"></a>
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=4HKjezHU-w8">
                <papertitle>Parameterizing Non-Parametric Meta-Reinforcement Learning Tasks via Subtask Decomposition </papertitle>
              </a>
              <br>
              <strong>Suyoung Lee</strong>, Myungsik Cho, and <a href="https://sites.google.com/view/youngchulsung">Youngchul Sung</a>

              <br>
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2023.
              <br>
              <a href="https://openreview.net/forum?id=JX6UloWrmE&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DNeurIPS.cc%2F2023%2FConference%2FAuthors%23your-submissions)">pdf</a>
              
              <p></p>
              <p> We enhance the generalization capability of meta-reinforcement learning on tasks with non-parametric variability by decomposing the tasks into elementary subtasks and conducting virtual training.</p>
            </td>
          </tr>
		
          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
		      <a href="images/AIMDA.png"><img src='images/AIMDA.png' width="190"></a>
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=4HKjezHU-w8">
                <papertitle>Adaptive Intrinsic Motivation with Decision Awareness </papertitle>
              </a>
              <br>
              <strong>Suyoung Lee</strong> and
							<a href="http://itl.kaist.ac.kr/sychung/index.html">Sae-Young Chung</a>

              <br>
              <em>Decision Awareness in Reinforcement Learning Workshop at ICML</em>, 2022.
              <br>
              <a href="https://openreview.net/pdf?id=4HKjezHU-w8">pdf</a>
              
              <p></p>
              <p> We propose an intrinsic reward coefficient adaptation scheme equipped with intrinsic motivation awareness and adjusts the intrinsic reward coefficient online to maximize the extrinsic return.</p>
            </td>
          </tr>

          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
		      <a href='images/LDM.png'> <img src='images/LDM.png' width="190"> </a>
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://proceedings.neurips.cc/paper/2021/hash/e48e13207341b6bffb7fb1622282247b-Abstract.html">
                <papertitle>Improving Generalization in Meta-RL with Imaginary Tasks from Latent Dynamics Mixture</papertitle>
              </a>
              <br>
              <strong>Suyoung Lee</strong> and
							<a href="http://itl.kaist.ac.kr/sychung/index.html">Sae-Young Chung</a>

              <br>
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2021.
              <br>
              <a href="https://arxiv.org/pdf/2105.13524.pdf">pdf</a>
              /
              <a href="https://github.com/suyoung-lee/LDM">code</a>
              <p></p>
              <p> We train an RL agent with imaginary tasks generated from mixtures of learned latent dynamics to generalize to unseen test tasks.</p>
            </td>
          </tr>

          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
		      <a href ='images/EBU.jpg'> <img src='images/EBU.jpg' width="190"></a>
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://proceedings.neurips.cc/paper/2019/hash/e6d8545daa42d5ced125a4bf747b3688-Abstract.html">
                <papertitle>Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update</papertitle>
              </a>
              <br>
              <strong>Suyoung Lee</strong>,
							<a href="https://www.linkedin.com/in/sungik-choi-171a05137/?originalSubdomain=kr">Sungik Choi</a>, and
							<a href="http://itl.kaist.ac.kr/sychung/index.html">Sae-Young Chung</a>

              <br>
              <em>Neural Information Processing Systems (NeurIPS)</em>, 2019.
              <br>
              <a href="https://arxiv.org/pdf/1805.12375.pdf">pdf</a>
              /
              <a href="https://github.com/suyoung-lee/Episodic-Backward-Update">code</a>
              <p></p>
              <p>A computationally efficient recursive deep reinforcement learning algorithm that allows sparse and delayed rewards to propagate directly through all transitions of the sampled episode.</p>
            </td>
          </tr>


        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <heading>Awards</heading>

          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">

              <p></p>
              <p>Qualcomm-KAIST Innovation Awards 2018 - paper competition awards for graduate students, Qualcomm, 2018. 2018.</p>
              <p>Un Chong-Kwan Scholarship Award - for achievement of excellence in 2017 entrance examination, KAIST EE, 2017.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <heading>Education</heading>

          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">

              <p></p>
              <p>2022~ current: Ph.D. in Electrical Engineering, KAIST, Daejeon, Korea (advisor: Prof. <a href="https://sites.google.com/view/youngchulsung">Youngchul Sung</a>). </p>
	      <p>2019~2022: Ph.D. in Electrical Engineering, KAIST, Daejeon, Korea (advisor: Prof. <a href="http://itl.kaist.ac.kr/sychung/index.html">Sae-Young Chung</a>). </p>
              <p>2017~2019: M.S. in Electrical Engineering, KAIST, Daejeon, Korea (advisor: Prof. <a href="http://itl.kaist.ac.kr/sychung/index.html">Sae-Young Chung</a>). </p>
              <p>2012~2017: B.S. in Electrical Engineering, KAIST, Daejeon, Korea. </p>
              <p>2010~2012: Hansung Science High School, Seoul, Korea. </p>
	      <p>2007~2009: Tashkent International School, Tashkent, Uzbekistan. </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <heading>Teaching</heading>

          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">

              <p></p>
		    <p>2020 fall : TA, EE326 Introduction to Information Theory and Coding, KAIST. </p>
		    <p>2020 spring : TA, EE210 Probability and Introductory Random Processes, KAIST. </p>
		    <p>2019 fall : TA, EE105 Electrical Engineering: Changing the World, KAIST. </p>
		    <p>2019 spring : TA, EE405 Electronics Design Lab. Network of Smart Things, KAIST. </p>
		    <p>2018 fall : TA, EE807 Special Topics in Electrical Engineering. Deep Reinforcement Learning and AlphaGo, KAIST. (Course rewarded with the outstanding TA award) </p>
		    <p>2018 spring : TA, EE405 Electronics Design Lab. Network of Smart Systems, KAIST. </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <heading>Academic Acivities</heading>

          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">

              <p></p>
              <p>Conference reviewer: ICML 2021-2023, NeurIPS 2021-2023</p>

            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <heading>How I try to live</heading>

          <tr onmouseout="hypernerf_stop()" onmouseover="hypernerf_start()">
	<td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
		      <a href="images/Life.jpg"><img src='images/Life.jpg' width="190"></a>
              </div>

            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">

              <p></p>
              <p>I view life as a meta-reinforcement learning task, reminiscent of the MuJoCo Ant-direction. Everyone has their own unique, albeit often obscured, optimal life direction $`\mathcal{T}`$. The objective of life is to maximize the cumulative reward, defined as the dot product of our chosen direction (how we decide to live) and the unseen true direction. I was fortunate to have guidance from two professors who instilled in me the importance of minimizing the angle 
∣θ∣ and maximizing the magnitude r.</p>

            </td>
          </tr>
        </tbody></table>
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website template from <a href="https://jonbarron.info/">here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>

      </td>
    </tr>
  </table>
</body>

</html>
